"""
Speedup Test for R2R vs Baseline on AIME25 Dataset

This script measures the speedup achieved by R2R method compared to baseline (large model only).
It uses the first 5 problems from the AIME25 dataset and runs each problem 5 times.

Metrics measured:
1. Average latency (wall clock time)
2. Token throughput (output tokens per second)
3. Large model ratio (percentage of tokens generated by large model)

Usage:
    python speedup_test_aime25.py --baseline_model_path <path> --router_path <path> [options]
"""

import os
os.environ['MASTER_ADDR'] = 'localhost'
os.environ.setdefault('MASTER_PORT', '29500')
os.environ['SGL_DISABLE_TP_MEMORY_INBALANCE_CHECK'] = '1'
os.environ["TRANSFORMERS_VERBOSITY"] = "error"

import time
import json
import argparse
import warnings
import asyncio
from datetime import datetime
from typing import List, Dict, Any
import statistics

import asyncio

import torch
import pandas as pd
import multiprocessing as mp
from datasets import load_dataset
from transformers import AutoTokenizer
import sglang as sgl

from r2r.models.dynamic_sglang_selector import DynamicSimpleSGLangSelector
from r2r.utils.config import MODEL_DICT, TOTAL_GPU_NUM

# Suppress warnings
warnings.filterwarnings("ignore")
torch.set_warn_always(False)


def load_dataset_config(dataset_name: str = "aime25") -> Dict:
    """Load dataset configuration from eval_configs/dataset_configs.json.

    Args:
        dataset_name: Name of the dataset configuration to load

    Returns:
        Dataset configuration dictionary
    """
    config_path = os.path.join(
        os.path.dirname(__file__),
        'script/evaluate/eval_configs/dataset_configs.json'
    )

    try:
        with open(config_path, 'r') as f:
            configs = json.load(f)

        if dataset_name not in configs:
            raise ValueError(f"Dataset '{dataset_name}' not found in dataset_configs.json")

        return configs[dataset_name]
    except FileNotFoundError:
        raise FileNotFoundError(f"Dataset configuration file not found at {config_path}")


def load_aime25_problems(num_problems: int = 5, dataset_name: str = "aime25") -> List[Dict[str, Any]]:
    """Load the first N problems from AIME25 dataset using configuration.

    Args:
        num_problems: Number of problems to load (default: 5)
        dataset_name: Dataset configuration name to use (default: "aime25")

    Returns:
        List of preprocessed problem dictionaries
    """
    print(f"Loading dataset configuration for '{dataset_name}'...")
    dataset_config = load_dataset_config(dataset_name)

    dataset_path = dataset_config['path']
    id_field = dataset_config['id_field']
    question_field = dataset_config['question_field']
    answer_field = dataset_config['answer_field']
    prompt_template = dataset_config['prompt_template']

    print(f"Loading dataset from '{dataset_path}'...")
    dataset = load_dataset(dataset_path, trust_remote_code=True)

    # Use the train split if available, otherwise test
    dataset_split = dataset['train'] if 'train' in dataset else dataset['test']

    # Get first N problems
    problems = []

    for i, item in enumerate(dataset_split):
        if i >= num_problems:
            break

        problem = {
            'ID': str(item.get(id_field, f'problem_{i}')),
            'Problem': item[question_field],
            'Answer': item[answer_field],
            'FormattedProblem': prompt_template.format(question=item[question_field])
        }
        problems.append(problem)

    print(f"Loaded {len(problems)} problems from {dataset_name}")
    print(f"Using prompt template: {prompt_template[:100]}...")
    return problems


class BaselineGenerator:
    """Baseline generator using only the large model."""

    def __init__(self, model_path: str, tp_size: int = 2):
        """Initialize baseline generator.

        Args:
            model_path: Path to the large model
            tp_size: Tensor parallelism size
        """
        self.model_path = model_path
        self.tp_size = tp_size

        print(f"Initializing baseline model: {model_path}")
        print(f"Using tensor parallelism size: {tp_size}")

        # Initialize sglang model (following hf_dataset_sglang.py pattern)
        self.engine = sgl.Engine(
            model_path=model_path,
            dtype="bfloat16",
            skip_tokenizer_init=False,  # Let SGLang handle tokenizer
            tp_size=tp_size,
            dp_size=1
        )

        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        print("Baseline model initialized successfully")

    def warmup(self):
        """Warmup the model to compile CUDA graphs and initialize GPU caches.

        This prevents the first benchmark run from including model loading overhead.
        """
        print("Warming up baseline model...")

        # Set up event loop for warmup
        try:
            asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)

        # Create a simple warmup prompt
        warmup_prompt = "Hi"
        messages = [{"role": "user", "content": warmup_prompt}]
        formatted_prompt = self.tokenizer.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )

        # Run a short generation to warmup
        sampling_params = {
            "max_new_tokens": 10,
            "temperature": 0.6,
            "top_p": 0.95,
            "top_k": 20,
        }

        _ = self.engine.generate([formatted_prompt], sampling_params=sampling_params)
        print("Baseline model warmup complete!")

    def generate(self, prompt: str, max_new_tokens: int = 32768,
                 temperature: float = 0.6, top_p: float = 0.95, top_k: int = 20) -> Dict[str, Any]:
        """Generate response for a single prompt.

        Args:
            prompt: Input prompt
            max_new_tokens: Maximum tokens to generate
            temperature: Sampling temperature
            top_p: Nucleus sampling parameter
            top_k: Top-k filtering parameter

        Returns:
            Dictionary containing generation results and metrics
        """
        # Set up event loop if needed
        try:
            asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)

        # Apply chat template
        messages = [{"role": "user", "content": prompt}]
        formatted_prompt = self.tokenizer.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )

        # Measure wall clock time
        start_time = time.time()

        sampling_params = {
            "max_new_tokens": max_new_tokens,
            "temperature": temperature,
            "top_p": top_p,
            "top_k": top_k,
        }

        # Generate with sglang - pass prompts directly as strings (not input_ids)
        generated_results = self.engine.generate([formatted_prompt], sampling_params=sampling_params)

        end_time = time.time()
        wall_clock_time = end_time - start_time

        # Extract generated text
        generated_text = generated_results[0]['text']

        # Calculate metrics
        input_tokens = len(self.tokenizer.encode(formatted_prompt))
        output_tokens = len(self.tokenizer.encode(generated_text))
        total_tokens = input_tokens + output_tokens
        token_throughput = output_tokens / wall_clock_time if wall_clock_time > 0 else 0

        return {
            'generated_text': generated_text,
            'wall_clock_time': wall_clock_time,
            'input_tokens': input_tokens,
            'output_tokens': output_tokens,
            'total_tokens': total_tokens,
            'token_throughput': token_throughput,
            'large_model_ratio': 100.0,  # Baseline uses 100% large model
        }

    def shutdown(self):
        """Shutdown the engine."""
        if hasattr(self.engine, 'shutdown'):
            self.engine.shutdown()


class R2RGenerator:
    """R2R generator using dynamic model selection."""

    def __init__(self, router_path: str, tp_size: int = 2, threshold: float = 0.40595959595959596):
        """Initialize R2R generator.

        Args:
            router_path: Path to the router model
            tp_size: Tensor parallelism size for reference model
            threshold: Neural router threshold
        """
        self.router_path = router_path
        self.tp_size = tp_size
        self.threshold = threshold

        print(f"Initializing R2R generator with router: {router_path}")
        print(f"Using tensor parallelism size: {tp_size}")
        print(f"Router threshold: {threshold}")

        strategy_kwargs = {
            'model_path': router_path,
            'threshold': threshold,
        }

        sglang_kwargs = {
            "dtype": "bfloat16",
            "tp_size": tp_size,
        }

        self.engine = DynamicSimpleSGLangSelector(
            device="cuda",
            dtype=torch.bfloat16,
            switching_strategy='neural',
            strategy_kwargs=strategy_kwargs,
            is_record=True,  # Enable recording to track model usage
            sglang_kwargs=sglang_kwargs
        )

        self.tokenizer = self.engine.tokenizer
        print("R2R generator initialized successfully")

    def warmup(self):
        """Warmup the R2R models to compile CUDA graphs and initialize GPU caches.

        This prevents the first benchmark run from including model loading overhead.
        Note: R2R already has internal warmup in DynamicSimpleSGLangSelector.__init__(),
        but we add an additional warmup here to ensure full readiness.
        """
        print("Warming up R2R models...")

        # Set up event loop for warmup
        try:
            asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)

        # Create a simple warmup prompt
        warmup_prompt = "Hi"
        messages = [{"role": "user", "content": warmup_prompt}]
        formatted_prompt = self.tokenizer.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )

        input_ids = [self.tokenizer.encode(formatted_prompt)]

        # Run a short generation to warmup
        _ = self.engine.generate(
            input_ids,
            max_new_tokens=10,
            temperature=0.6,
            top_p=0.95,
            top_k=20,
            record_generation=False,
            print_tokens=False,
        )
        print("R2R models warmup complete!")

    def generate(self, prompt: str, max_new_tokens: int = 32768,
                 temperature: float = 0.6, top_p: float = 0.95, top_k: int = 20) -> Dict[str, Any]:
        """Generate response for a single prompt.

        Args:
            prompt: Input prompt
            max_new_tokens: Maximum tokens to generate
            temperature: Sampling temperature
            top_p: Nucleus sampling parameter
            top_k: Top-k filtering parameter

        Returns:
            Dictionary containing generation results and metrics
        """
        # Set up event loop if needed
        try:
            asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)

        # Apply chat template
        messages = [{"role": "user", "content": prompt}]
        formatted_prompt = self.tokenizer.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )

        input_ids = [self.tokenizer.encode(formatted_prompt)]

        # Measure wall clock time
        start_time = time.time()

        # Generate with recording enabled
        generated_texts, recorders = self.engine.generate(
            input_ids,
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            top_p=top_p,
            top_k=top_k,
            record_generation=True,
            print_tokens=False,
        )

        end_time = time.time()
        wall_clock_time = end_time - start_time

        # Extract generated text (strip prompt)
        generated_text_full = generated_texts[0]
        generated_text = self.tokenizer.decode(
            self.tokenizer.encode(generated_text_full)[len(input_ids[0]):],
            skip_special_tokens=True,
        )

        # Get statistics from recorder
        recorder = recorders[0]
        stats = recorder.get_statistics()

        # Calculate metrics
        input_tokens = len(input_ids[0])
        output_tokens = len(self.tokenizer.encode(generated_text_full)) - input_tokens
        total_tokens = input_tokens + output_tokens
        token_throughput = output_tokens / wall_clock_time if wall_clock_time > 0 else 0

        return {
            'generated_text': generated_text,
            'wall_clock_time': wall_clock_time,
            'input_tokens': input_tokens,
            'output_tokens': output_tokens,
            'total_tokens': total_tokens,
            'token_throughput': token_throughput,
            'large_model_ratio': stats['reference_model_percentage'],
            'quick_model_ratio': stats['quick_model_percentage'],
            'model_agreement_percentage': stats['model_agreement_percentage'],
        }

    def shutdown(self):
        """Shutdown the engine."""
        if hasattr(self.engine, 'shutdown'):
            self.engine.shutdown()


def run_benchmark(problems: List[Dict], generator: Any, num_runs: int = 5,
                  max_new_tokens: int = 32768, temperature: float = 0.6,
                  top_p: float = 0.95, top_k: int = 20) -> List[Dict]:
    """Run benchmark on problems using the given generator.

    Args:
        problems: List of problem dictionaries
        generator: BaselineGenerator or R2RGenerator
        num_runs: Number of runs per problem
        max_new_tokens: Maximum tokens to generate
        temperature: Sampling temperature
        top_p: Nucleus sampling parameter
        top_k: Top-k filtering parameter

    Returns:
        List of result dictionaries
    """
    results = []

    for problem_idx, problem in enumerate(problems):
        print(f"\nProcessing problem {problem_idx + 1}/{len(problems)}: {problem['ID']}")

        for run_idx in range(num_runs):
            print(f"  Run {run_idx + 1}/{num_runs}...", end=' ', flush=True)

            result = generator.generate(
                prompt=problem['FormattedProblem'],
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                top_p=top_p,
                top_k=top_k,
            )

            result.update({
                'problem_id': problem['ID'],
                'problem_index': problem_idx,
                'run_index': run_idx,
                'problem': problem['Problem'],
                'answer': problem['Answer'],
            })

            results.append(result)

            print(f"Done! Time: {result['wall_clock_time']:.2f}s, "
                  f"Throughput: {result['token_throughput']:.2f} tok/s, "
                  f"LLM ratio: {result['large_model_ratio']:.1f}%")

    return results


def calculate_summary_metrics(baseline_results: List[Dict],
                              r2r_results: List[Dict]) -> Dict[str, Any]:
    """Calculate summary metrics and speedup.

    Args:
        baseline_results: Results from baseline generator
        r2r_results: Results from R2R generator

    Returns:
        Dictionary of summary metrics
    """
    # Calculate baseline metrics
    baseline_latencies = [r['wall_clock_time'] for r in baseline_results]
    baseline_throughputs = [r['token_throughput'] for r in baseline_results]
    baseline_output_tokens = [r['output_tokens'] for r in baseline_results]

    # Calculate R2R metrics
    r2r_latencies = [r['wall_clock_time'] for r in r2r_results]
    r2r_throughputs = [r['token_throughput'] for r in r2r_results]
    r2r_output_tokens = [r['output_tokens'] for r in r2r_results]
    r2r_large_model_ratios = [r['large_model_ratio'] for r in r2r_results]

    # Calculate averages
    avg_baseline_latency = statistics.mean(baseline_latencies)
    avg_baseline_throughput = statistics.mean(baseline_throughputs)
    avg_baseline_output_tokens = statistics.mean(baseline_output_tokens)

    avg_r2r_latency = statistics.mean(r2r_latencies)
    avg_r2r_throughput = statistics.mean(r2r_throughputs)
    avg_r2r_output_tokens = statistics.mean(r2r_output_tokens)
    avg_r2r_large_model_ratio = statistics.mean(r2r_large_model_ratios)

    # Calculate speedup
    latency_speedup = avg_baseline_latency / avg_r2r_latency if avg_r2r_latency > 0 else 0
    throughput_speedup = avg_r2r_throughput / avg_baseline_throughput if avg_baseline_throughput > 0 else 0

    summary = {
        'baseline': {
            'avg_latency_sec': avg_baseline_latency,
            'avg_throughput_tok_per_sec': avg_baseline_throughput,
            'avg_output_tokens': avg_baseline_output_tokens,
            'total_runs': len(baseline_results),
        },
        'r2r': {
            'avg_latency_sec': avg_r2r_latency,
            'avg_throughput_tok_per_sec': avg_r2r_throughput,
            'avg_output_tokens': avg_r2r_output_tokens,
            'avg_large_model_ratio_percent': avg_r2r_large_model_ratio,
            'total_runs': len(r2r_results),
        },
        'speedup': {
            'latency_speedup': latency_speedup,
            'throughput_speedup': throughput_speedup,
            'latency_improvement_percent': (latency_speedup - 1) * 100,
            'throughput_improvement_percent': (throughput_speedup - 1) * 100,
        }
    }

    return summary


def print_summary_table(summary: Dict[str, Any]):
    """Print formatted summary table to console.

    Args:
        summary: Summary metrics dictionary
    """
    print("\n" + "="*80)
    print("SPEEDUP TEST SUMMARY - AIME25")
    print("="*80)

    print("\nBASELINE (Large Model Only):")
    print("-" * 80)
    print(f"  Average Latency:          {summary['baseline']['avg_latency_sec']:.3f} seconds")
    print(f"  Average Throughput:       {summary['baseline']['avg_throughput_tok_per_sec']:.2f} tokens/sec")
    print(f"  Average Output Tokens:    {summary['baseline']['avg_output_tokens']:.1f}")
    print(f"  Total Runs:               {summary['baseline']['total_runs']}")

    print("\nR2R (Dynamic Model Selection):")
    print("-" * 80)
    print(f"  Average Latency:          {summary['r2r']['avg_latency_sec']:.3f} seconds")
    print(f"  Average Throughput:       {summary['r2r']['avg_throughput_tok_per_sec']:.2f} tokens/sec")
    print(f"  Average Output Tokens:    {summary['r2r']['avg_output_tokens']:.1f}")
    print(f"  Average Large Model Ratio: {summary['r2r']['avg_large_model_ratio_percent']:.2f}%")
    print(f"  Total Runs:               {summary['r2r']['total_runs']}")

    print("\nSPEEDUP ACHIEVED:")
    print("-" * 80)
    print(f"  Latency Speedup:          {summary['speedup']['latency_speedup']:.2f}x")
    print(f"  Throughput Speedup:       {summary['speedup']['throughput_speedup']:.2f}x")
    print(f"  Latency Improvement:      {summary['speedup']['latency_improvement_percent']:.2f}%")
    print(f"  Throughput Improvement:   {summary['speedup']['throughput_improvement_percent']:.2f}%")

    print("\n" + "="*80)


def save_results(baseline_results: List[Dict], r2r_results: List[Dict],
                summary: Dict[str, Any], output_dir: str):
    """Save results to JSON and CSV files.

    Args:
        baseline_results: Results from baseline generator
        r2r_results: Results from R2R generator
        summary: Summary metrics
        output_dir: Directory to save results
    """
    os.makedirs(output_dir, exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # Save detailed results to JSON
    detailed_results = {
        'baseline_results': baseline_results,
        'r2r_results': r2r_results,
        'summary': summary,
        'timestamp': timestamp,
    }

    json_path = os.path.join(output_dir, f"speedup_test_detailed_{timestamp}.json")
    with open(json_path, 'w') as f:
        json.dump(detailed_results, f, indent=2, default=str)
    print(f"\nDetailed results saved to: {json_path}")

    # Save baseline results to CSV
    baseline_df = pd.DataFrame(baseline_results)
    baseline_csv_path = os.path.join(output_dir, f"baseline_results_{timestamp}.csv")
    baseline_df.to_csv(baseline_csv_path, index=False)
    print(f"Baseline results CSV saved to: {baseline_csv_path}")

    # Save R2R results to CSV
    r2r_df = pd.DataFrame(r2r_results)
    r2r_csv_path = os.path.join(output_dir, f"r2r_results_{timestamp}.csv")
    r2r_df.to_csv(r2r_csv_path, index=False)
    print(f"R2R results CSV saved to: {r2r_csv_path}")

    # Save summary to CSV
    summary_df = pd.DataFrame([{
        'metric': 'baseline_avg_latency_sec',
        'value': summary['baseline']['avg_latency_sec']
    }, {
        'metric': 'baseline_avg_throughput_tok_per_sec',
        'value': summary['baseline']['avg_throughput_tok_per_sec']
    }, {
        'metric': 'r2r_avg_latency_sec',
        'value': summary['r2r']['avg_latency_sec']
    }, {
        'metric': 'r2r_avg_throughput_tok_per_sec',
        'value': summary['r2r']['avg_throughput_tok_per_sec']
    }, {
        'metric': 'r2r_avg_large_model_ratio_percent',
        'value': summary['r2r']['avg_large_model_ratio_percent']
    }, {
        'metric': 'latency_speedup',
        'value': summary['speedup']['latency_speedup']
    }, {
        'metric': 'throughput_speedup',
        'value': summary['speedup']['throughput_speedup']
    }])

    summary_csv_path = os.path.join(output_dir, f"speedup_summary_{timestamp}.csv")
    summary_df.to_csv(summary_csv_path, index=False)
    print(f"Summary CSV saved to: {summary_csv_path}")


def main():
    parser = argparse.ArgumentParser(
        description="Speedup test for R2R vs Baseline on AIME25 dataset"
    )

    # Model paths
    parser.add_argument('--baseline_model_path', type=str,
                       default='Qwen/Qwen3-32B',
                       help='Path to baseline large model (default: Qwen/Qwen3-32B)')
    parser.add_argument('--router_path', type=str,
                       default='resource/qwen3_1.7_8_router.pt',
                       help='Path to R2R router model (default: resource/default_router.pt)')

    # Test configuration
    parser.add_argument('--dataset', type=str, default='aime25',
                       help='Dataset configuration to use from dataset_configs.json (default: aime25)')
    parser.add_argument('--num_problems', type=int, default=5,
                       help='Number of problems to test (default: 5)')
    parser.add_argument('--num_runs', type=int, default=5,
                       help='Number of runs per problem (default: 5)')

    # Generation parameters
    parser.add_argument('--max_new_tokens', type=int, default=32768,
                       help='Maximum tokens to generate (default: 2048)')
    parser.add_argument('--temperature', type=float, default=0.6,
                       help='Sampling temperature (default: 0.6)')
    parser.add_argument('--top_p', type=float, default=0.95,
                       help='Nucleus sampling top_p (default: 0.95)')
    parser.add_argument('--top_k', type=int, default=20,
                       help='Top-k filtering parameter (default: 20)')

    # Hardware configuration
    parser.add_argument('--tp_size', type=int, default=2,
                       help='Tensor parallelism size (default: 2)')

    # R2R configuration
    parser.add_argument('--threshold', type=float, default=0.40595959595959596,
                       help='Neural router threshold (default: 0.9)')

    # Output
    parser.add_argument('--output_dir', type=str,
                       default='output/speedup_test_aime25',
                       help='Directory to save results (default: output/speedup_test_aime25)')

    # Test mode
    parser.add_argument('--test_mode', type=str, default='both',
                       choices=['baseline', 'r2r', 'both'],
                       help='Which test to run: baseline, r2r, or both (default: both)')

    args = parser.parse_args()

    print("="*80)
    print("SPEEDUP TEST: R2R vs Baseline")
    print("="*80)
    print(f"Test Configuration:")
    print(f"  Dataset:            {args.dataset}")
    print(f"  Number of problems: {args.num_problems}")
    print(f"  Runs per problem:   {args.num_runs}")
    print(f"  Max new tokens:     {args.max_new_tokens}")
    print(f"  Temperature:        {args.temperature}")
    print(f"  Top-p:              {args.top_p}")
    print(f"  Top-k:              {args.top_k}")
    print(f"  Tensor parallel:    {args.tp_size}")
    print(f". Threshold:          {args.threshold}")
    print("="*80)

    # Load problems using dataset configuration
    problems = load_aime25_problems(num_problems=args.num_problems, dataset_name=args.dataset)

    baseline_results = []
    r2r_results = []

    # Run baseline benchmark
    if args.test_mode in ['baseline', 'both']:
        print("\n" + "="*80)
        print("RUNNING BASELINE BENCHMARK (Large Model Only)")
        print("="*80)

        baseline_generator = BaselineGenerator(
            model_path=args.baseline_model_path,
            tp_size=args.tp_size
        )

        # Warmup the model before benchmarking
        baseline_generator.warmup()

        baseline_results = run_benchmark(
            problems=problems,
            generator=baseline_generator,
            num_runs=args.num_runs,
            max_new_tokens=args.max_new_tokens,
            temperature=args.temperature,
            top_p=args.top_p,
            top_k=args.top_k,
        )

        baseline_generator.shutdown()
        print("\nBaseline benchmark completed!")

    # Run R2R benchmark
    if args.test_mode in ['r2r', 'both']:
        print("\n" + "="*80)
        print("RUNNING R2R BENCHMARK (Dynamic Model Selection)")
        print("="*80)

        r2r_generator = R2RGenerator(
            router_path=args.router_path,
            tp_size=args.tp_size,
            threshold=args.threshold,
        )

        # Warmup the model before benchmarking
        r2r_generator.warmup()

        r2r_results = run_benchmark(
            problems=problems,
            generator=r2r_generator,
            num_runs=args.num_runs,
            max_new_tokens=args.max_new_tokens,
            temperature=args.temperature,
            top_p=args.top_p,
            top_k=args.top_k,
        )

        r2r_generator.shutdown()
        print("\nR2R benchmark completed!")

    # Calculate and display summary
    if args.test_mode == 'both' and baseline_results and r2r_results:
        summary = calculate_summary_metrics(baseline_results, r2r_results)
        print_summary_table(summary)
        save_results(baseline_results, r2r_results, summary, args.output_dir)
    elif args.test_mode == 'baseline' and baseline_results:
        print("\nBaseline results saved (run with --test_mode both to calculate speedup)")
        os.makedirs(args.output_dir, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        baseline_df = pd.DataFrame(baseline_results)
        baseline_csv_path = os.path.join(args.output_dir, f"baseline_results_{timestamp}.csv")
        baseline_df.to_csv(baseline_csv_path, index=False)
        print(f"Baseline results saved to: {baseline_csv_path}")
    elif args.test_mode == 'r2r' and r2r_results:
        print("\nR2R results saved (run with --test_mode both to calculate speedup)")
        os.makedirs(args.output_dir, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        r2r_df = pd.DataFrame(r2r_results)
        r2r_csv_path = os.path.join(args.output_dir, f"r2r_results_{timestamp}.csv")
        r2r_df.to_csv(r2r_csv_path, index=False)
        print(f"R2R results saved to: {r2r_csv_path}")


if __name__ == "__main__":
    if torch.cuda.is_available():
        print(f"CUDA available with {torch.cuda.device_count()} GPUs")
    else:
        print("WARNING: CUDA not available")

    mp.set_start_method("spawn", force=True)
    
    try:
        import uvloop
        asyncio.set_event_loop_policy(uvloop.EventLoopPolicy())
    except ImportError:
        pass

    try:
        main()
    except Exception as e:
        print(f"An error occurred: {e}")
        import traceback
        traceback.print_exc()
