# AIME25 Speedup Test

This script measures the speedup achieved by R2R method compared to a baseline (large model only) on the AIME25 dataset.

## Features

- **Dataset**: Uses the first 5 problems from AIME25 dataset (configurable)
- **Repetition**: Runs each problem 5 times for statistical reliability (configurable)
- **Metrics Measured**:
  1. **Average Latency**: Wall clock time per generation (seconds)
  2. **Token Throughput**: Output tokens generated per second (tokens/sec)
  3. **Large Model Ratio**: Percentage of tokens generated by the large model (%)

## Default Models

The script uses the following models by default:

- **Baseline Model**: `Qwen/Qwen3-32B` (32B parameters)
- **R2R Quick Model**: `Qwen/Qwen3-1.7B` (1.7B parameters) - configured in MODEL_DICT
- **R2R Reference Model**: `Qwen/Qwen3-32B` (32B parameters) - configured in MODEL_DICT

These models will be automatically downloaded from HuggingFace when first run.

## Requirements

- Python 3.8+
- PyTorch with CUDA support
- Transformers
- Datasets
- SGLang
- Pandas
- At least 2 GPUs (for R2R mode with TP)
- Internet connection (for downloading models from HuggingFace on first run)

## Installation

```bash
# Ensure you have the R2R-Experiment-for-RelayGen environment set up
cd R2R-Experiment-for-RelayGen
# Install dependencies if needed
pip install -r requirements.txt  # if you have a requirements file
```

## Usage

### Basic Usage (Test Both Baseline and R2R with Default Models)

```bash
# Uses default models: Qwen/Qwen3-32B for baseline, Qwen/Qwen3-1.7B & Qwen/Qwen3-32B for R2R
python speedup_test_aime25.py

# Or use the convenience script
./run_speedup_test.sh
```

### Using Different Models

```bash
# Example: Use DeepSeek-R1 as baseline instead
python speedup_test_aime25.py \
    --baseline_model_path deepseek-ai/DeepSeek-R1-Distill-Qwen-32B

# Example: Use a different router and adjust threshold
python speedup_test_aime25.py \
    --router_path path/to/your/custom_router.pt \
    --threshold 0.85

# Full example with all custom parameters
python speedup_test_aime25.py \
    --baseline_model_path deepseek-ai/DeepSeek-R1-Distill-Qwen-32B \
    --router_path resource/custom_router.pt \
    --num_problems 5 \
    --num_runs 5 \
    --max_new_tokens 2048 \
    --temperature 0.6 \
    --top_p 0.95 \
    --tp_size 2 \
    --threshold 0.9 \
    --output_dir output/speedup_test_aime25
```

### Changing R2R Models

To change the quick and reference models used by R2R, edit the [r2r/utils/model_configs.json](r2r/utils/model_configs.json) file:

```json
{
  "quick": {
    "model_name": "Your-Quick-Model-Name",
    "model_path": "huggingface/model-path",
    "param": "1.7",
    "mem_fraction_static": 0.15
  },
  "reference": {
    "model_name": "Your-Reference-Model-Name",
    "model_path": "huggingface/model-path",
    "param": "32",
    "mem_fraction_static": 0.80
  }
}
```

### Test Only Baseline

```bash
python speedup_test_aime25.py --test_mode baseline
```

### Test Only R2R

```bash
python speedup_test_aime25.py --test_mode r2r
```

## Command Line Arguments

### Model Arguments

- `--baseline_model_path`: Path to the baseline large model (default: `Qwen/Qwen3-32B`)
- `--router_path`: Path to the R2R router model (default: `resource/default_router.pt`)

### Optional Test Configuration

**Test Configuration:**
- `--num_problems`: Number of problems to test (default: 5)
- `--num_runs`: Number of runs per problem (default: 5)
- `--test_mode`: Which test to run - `baseline`, `r2r`, or `both` (default: `both`)

**Generation Parameters:**
- `--max_new_tokens`: Maximum tokens to generate (default: 2048)
- `--temperature`: Sampling temperature (default: 0.6)
- `--top_p`: Nucleus sampling top_p (default: 0.95)

**Hardware Configuration:**
- `--tp_size`: Tensor parallelism size (default: 2)

**R2R Configuration:**
- `--threshold`: Neural router threshold (default: 0.9)

**Output:**
- `--output_dir`: Directory to save results (default: `output/speedup_test_aime25`)

## Output Files

The script generates the following output files in the specified output directory:

1. **`speedup_test_detailed_<timestamp>.json`**: Detailed results including all individual runs
2. **`baseline_results_<timestamp>.csv`**: CSV with baseline run-by-run results
3. **`r2r_results_<timestamp>.csv`**: CSV with R2R run-by-run results
4. **`speedup_summary_<timestamp>.csv`**: Summary metrics and speedup calculations

## Output Metrics

### Console Output

The script prints a formatted summary table showing:

**Baseline Metrics:**
- Average latency (seconds)
- Average throughput (tokens/sec)
- Average output tokens
- Total runs

**R2R Metrics:**
- Average latency (seconds)
- Average throughput (tokens/sec)
- Average output tokens
- Average large model ratio (%)
- Total runs

**Speedup Achieved:**
- Latency speedup (X times faster)
- Throughput speedup (X times higher)
- Percentage improvements

### CSV/JSON Metrics

Per-run metrics include:
- `problem_id`: Problem identifier
- `problem_index`: Problem index (0-4 for first 5 problems)
- `run_index`: Run index (0-4 for 5 runs)
- `wall_clock_time`: Wall clock latency in seconds
- `input_tokens`: Number of input tokens
- `output_tokens`: Number of output tokens
- `total_tokens`: Total tokens (input + output)
- `token_throughput`: Output tokens per second
- `large_model_ratio`: Percentage of tokens from large model (100% for baseline, varies for R2R)
- `generated_text`: The generated response

R2R-specific metrics:
- `quick_model_ratio`: Percentage of tokens from quick (small) model
- `model_agreement_percentage`: Percentage agreement between models

## Example Output

```
================================================================================
SPEEDUP TEST SUMMARY - AIME25
================================================================================

BASELINE (Large Model Only):
--------------------------------------------------------------------------------
  Average Latency:          45.234 seconds
  Average Throughput:       42.15 tokens/sec
  Average Output Tokens:    1906.2
  Total Runs:               25

R2R (Dynamic Model Selection):
--------------------------------------------------------------------------------
  Average Latency:          12.567 seconds
  Average Throughput:       151.23 tokens/sec
  Average Output Tokens:    1900.8
  Average Large Model Ratio: 35.67%
  Total Runs:               25

SPEEDUP ACHIEVED:
--------------------------------------------------------------------------------
  Latency Speedup:          3.60x
  Throughput Speedup:       3.59x
  Latency Improvement:      259.89%
  Throughput Improvement:   258.79%

================================================================================
```

## Notes

- The script uses wall clock time measurement for accurate latency tracking
- Token throughput is calculated as output tokens per second (not including input tokens)
- Large model ratio shows what percentage of generated tokens came from the large model
  - Baseline: Always 100% (uses only large model)
  - R2R: Typically 20-50% depending on problem complexity and router threshold
- Lower large model ratio in R2R means more efficient use of the small model
- The speedup comes from R2R's ability to use the smaller, faster model for easier tokens

## Troubleshooting

**Out of Memory Errors:**
- Reduce `--max_new_tokens`
- Reduce `--tp_size` if you have fewer GPUs
- Reduce `--num_problems` or `--num_runs`

**Model Loading Issues:**
- Ensure the `--baseline_model_path` and `--router_path` are correct
- Check that you have sufficient GPU memory
- Verify the models are compatible with the current environment

**Performance Issues:**
- First run may be slower due to model loading and compilation
- Consider running a warmup (the code includes automatic warmup)
- Ensure CUDA is available and GPUs are not being used by other processes
